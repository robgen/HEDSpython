{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/robgen/HEDSpython/blob/main/Tutorial_6.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "UlJl3QrMWBz9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_R3gL_t-6GQ"
      },
      "source": [
        "#**Unsupervised Learning: PCA and k-means clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snTDgsTZye_T"
      },
      "source": [
        "In this tutorial we will go over two applications of unsupervised learning Principal Component Analysis and K-Means Clustering.\n",
        "\n",
        "Unsupervised learning is a type of machine learning that learns from data without human supervision. Results from unsupervised learning usually require interpretation ex-post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrA5Ojkr--yy"
      },
      "source": [
        "Let's import our usual libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqBX4Lh8-0du"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCQKjkDL_DCp"
      },
      "source": [
        "And load our csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy8ku0EK_G7-"
      },
      "outputs": [],
      "source": [
        "csvFilePath = 'https://raw.githubusercontent.com/robgen/HEDSpython/refs/heads/main/files/WVS_Cross-National_Wave_7_csv_v4_0.csv'\n",
        "rawData = pd.read_csv(csvFilePath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " rawData"
      ],
      "metadata": {
        "id": "rK9c7Ov0-nbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgveO6oT_pel"
      },
      "source": [
        "Let's check what the initial shape of our dataframe is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrIrpFk9_X-f"
      },
      "outputs": [],
      "source": [
        "print('the initial shape of the dataset is:', rawData.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzGqLZTF9Hf5"
      },
      "source": [
        " Now let's take a look at the column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAudiFT08ZXx"
      },
      "outputs": [],
      "source": [
        "print(rawData.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nexrbZ2u_2_q"
      },
      "source": [
        "To select the features which we need in our analysis we take a look at the *WVS questionnaire.pdf* file in th GitHub repository *files* folder to identify the relevant questions.\n",
        "\n",
        "In this case, we are interested in the 290 questions of the main survey. We know these are registered as *Q* + *question number* so we proceed to subset the dataframe to contain only the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB4F7Q4rALRl"
      },
      "outputs": [],
      "source": [
        "Nquestions = 290\n",
        "featuresToKeep = []\n",
        "for q in range(1,Nquestions+1):\n",
        "    featuresToKeep.append('Q'+str(q))\n",
        "\n",
        "WVS = rawData.loc[:, featuresToKeep]\n",
        "\n",
        "print('the final shape of the dataset is:', WVS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WVS"
      ],
      "metadata": {
        "id": "d0W3TmGK9uI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYqHFZ4wBFdk"
      },
      "source": [
        "We also want to retain some general features for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hShBT4kMBIN3"
      },
      "outputs": [],
      "source": [
        "generalFeatures = ['B_COUNTRY_ALPHA', 'O1_LONGITUDE', 'O2_LATITUDE']\n",
        "WVSgeneral = rawData.loc[:, generalFeatures]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqBR-LOCA18U"
      },
      "source": [
        "We can now delete the del `rawData` from the local memory (as we won't need it anymore and it takes up quite a lot of memory!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA2HSjZOA7Cp"
      },
      "outputs": [],
      "source": [
        "del rawData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVvrRJ0eBWe4"
      },
      "source": [
        "To get familiar with the data, let's plot a histogram of the observations by country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njmoya0oBZuk"
      },
      "outputs": [],
      "source": [
        "countries, numSurveysInCountry = np.unique(WVSgeneral.B_COUNTRY_ALPHA,return_counts=True)\n",
        "dummy = range(len(countries))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.rcParams.update({'font.size': 6})\n",
        "plt.bar(dummy,numSurveysInCountry, align='center')\n",
        "plt.xticks(dummy, countries)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDIDKq12CT88"
      },
      "source": [
        "Now let's clean the dataset. For each question we want to check which is the most common value and how many null values there are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nambbY8xCnQU"
      },
      "outputs": [],
      "source": [
        "nanQuestions = []\n",
        "weirdQuestions =[]\n",
        "for q in range(1,Nquestions+1):\n",
        "    labels, counts = np.unique(WVS['Q'+str(q)],return_counts=True)\n",
        "    print('Q'+str(q)+' - ', 'Most common: ' , labels[counts.argmax()], ',',\n",
        "            '#Empty: ', WVS['Q'+str(q)].isna().sum())\n",
        "    if np.isnan(labels[counts.argmax()]):\n",
        "        nanQuestions.append('Q' + str(q)) # append questions for which the most common value is nan\n",
        "    if labels[counts.argmax()] > 10 or labels[counts.argmax()] < 0:\n",
        "        weirdQuestions.append('Q'+str(q)) # append questions for which the most common value is above 10 or below 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nanQuestions"
      ],
      "metadata": {
        "id": "ikSIs9sW-ddU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weirdQuestions"
      ],
      "metadata": {
        "id": "GowSVh2JBmE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the questionnaire pdf in the files folder to check weird values. Q261 is the year of birth, Q262 is age so it is likely these values are above 10 (i.e., they do not look weird), while Q266 and Q272 are respecively out of 2 and 5 in the pdf, so they should not take values above 10 (i.e., they look weird). We re-define weird values accordingly."
      ],
      "metadata": {
        "id": "v-kf_sdHBn7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weirdQuestions = ['Q266', 'Q272']"
      ],
      "metadata": {
        "id": "V4sXMzQMDCNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD-QjuVnDhKS"
      },
      "source": [
        "We can then delete questions for which most common values are null or weird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Ji2TP_D2RW"
      },
      "outputs": [],
      "source": [
        "questionsToRemove = nanQuestions + weirdQuestions\n",
        "WVS = WVS.drop(questionsToRemove, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Z3Hm7YD6Ko"
      },
      "source": [
        "And then substitute `NaN` values with the most frequent answer for each question (the mode). PCA cannot be applied to a dataset with null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSfuM9XCe77w"
      },
      "outputs": [],
      "source": [
        "for name, values in WVS.items():\n",
        "    labels, counts = np.unique(values,return_counts=True)\n",
        "    values[values.isna()] = labels[counts.argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL90jGtYHSSA"
      },
      "source": [
        "##**Principal Component Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE8OeM06GCo0"
      },
      "source": [
        "We use the `sklearn` library to run our analysis. `sklearn` (or scikit-learn) is a powerful library for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxEm9owF8aZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daMMR7alENlD"
      },
      "source": [
        "Principal component analysis (PCA) is a dimensionality reduction technique that allows us to reduce a large set fo variables into a smaller one (called principal components) that still contains most of the information of the large dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDe3Db4VEt9r"
      },
      "source": [
        "As a first step we need to standardize all variables in the dataset to make sure that each of the variables contribute equally in the analysis.\n",
        "Standardizing a variable means to reduce it to a unit scale (that is to mean = 0 and variance = 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5mCg8VnG551"
      },
      "outputs": [],
      "source": [
        "standWVS = StandardScaler().fit_transform(WVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_A0JxNFThE"
      },
      "source": [
        "We use the `PCA()` and `.fit_transform()` functions to preform the reduction into a lower dimensional space, the argument `n_components` indicates how many components the variables should be reduced to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpPBtM1GHY1_"
      },
      "source": [
        "To start with, let's reduce the variables to two principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_0rRSNZPXOm"
      },
      "outputs": [],
      "source": [
        "pcaObj = PCA(n_components=2)\n",
        "prComp = pcaObj.fit_transform(standWVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaURA6dHhGH"
      },
      "source": [
        "Let's take a look at the two components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlkiE-z8PaDF"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=100)\n",
        "plt.scatter(prComp[:,0], prComp[:,1])\n",
        "plt.xlabel('PC1 [-] VarExp=%1.2f' %pcaObj.explained_variance_ratio_[0])\n",
        "plt.ylabel('PC2 [-] VarExp=%1.2f' %pcaObj.explained_variance_ratio_[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFfIcMpJBY4g"
      },
      "source": [
        "PC1 explains the variation on the x-axis while PC2 explains the variation on the y-axis. From this plot we can see already that PC1 explains more variance in the original dataset than PC2 as the scatter is more spread along the x-axis than the y-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEDs__ioCjqo"
      },
      "source": [
        "Let's now take a look at the loadings for each component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-6U46R2Jff0"
      },
      "outputs": [],
      "source": [
        "df_pca = pd.DataFrame(pcaObj.components_.T, columns=['PC1', 'PC2'])\n",
        "print(df_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiAlCJt8C1QI"
      },
      "source": [
        "We can use the largest and smallest loadings to interpret our components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOtN_KyOKxPa"
      },
      "outputs": [],
      "source": [
        "print(df_pca['PC1'].nlargest(2))\n",
        "print(df_pca['PC1'].nsmallest(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKVLUxZNBtH"
      },
      "source": [
        "**PC1 interpretation**\n",
        "\n",
        "At the *higher end* of the spectrum sit respondents that declared that 'Stealing property' (Q179) and 'Prostitution' (Q183) can be morally justified, that completely disagree with the statement 'One of the bad effects of science is that it breaks down people’s ideas of right and wrong' (Q161) and that have done the action 'Encouraging others to take action about political issues' (Q215).\n",
        "\n",
        "While at *lower end* of the spectrum stand people who do not morally justify 'Stealing property' (Q179) and 'Prostitution' (Q183), that agree that 'One of the bad effects of science is that it breaks down people’s ideas of right and wrong' (Q161) and that would never 'encourage others to take action about political issues' (Q215).\n",
        "\n",
        "What would you say is this PC measuring? Take a guess, there is no right or wrong answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7mjOe6bMLkQ"
      },
      "outputs": [],
      "source": [
        "print(df_pca['PC2'].nlargest(2))\n",
        "print(df_pca['PC2'].nsmallest(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEzz4qSAOLfF"
      },
      "source": [
        "**PC2 interpretation**\n",
        "\n",
        "At the *higher end* of the spectrum are people who do not have a lot of confidence in 'the court' (Q70) and in 'political parties' (Q72) and that think that 'The state makes people’s incomes equal' (Q247) and 'Civil rights protect people from state oppression' (Q246) are not essential characteristics of democracy.\n",
        "\n",
        "To the contrary, people with *lower scores* of PC2 will have a lot of confidence in 'the court' (Q70) and in 'political parties' (Q72) and will think that 'The state makes people’s incomes equal' (Q247) and 'Civil rights protect people from state oppression' (Q246) are more essential characteristics of democracy.\n",
        "\n",
        "How do you interpret this PC?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUg6Dot_QReF"
      },
      "source": [
        "The rule of thumb for PCA is that to have a good summary of the data the cumulative explained variance needs to exceed 70-80% of the variance.\n",
        "\n",
        "Let's check how much variance is explained by PC1 and PC2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ-6lP5jQ7ME"
      },
      "outputs": [],
      "source": [
        "pcaObj.explained_variance_ratio_.cumsum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7_uYI0-RHHp"
      },
      "source": [
        "Only 12%! Not a lot. Let's try with three components then."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hwDSvMOPdYE"
      },
      "outputs": [],
      "source": [
        "pcaObj = PCA(n_components=3)\n",
        "prComp = pcaObj.fit_transform(standWVS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K61iV2sLRiCQ"
      },
      "outputs": [],
      "source": [
        "pcaObj.explained_variance_ratio_.cumsum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mateyjFRk5V"
      },
      "source": [
        "Three PCs explain only 16% of the variance, that's not a lot either. We can still plot this to see what adding a third dimension looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRYDnu7zPfPW"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, figsize=(4, 3), dpi=200)\n",
        "plt.clf()\n",
        "\n",
        "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
        "ax.set_position([0, 0, 0.95, 1])\n",
        "plt.cla()\n",
        "ax.scatter(prComp[:,0], prComp[:,1], prComp[:,2], edgecolor=\"k\")\n",
        "ax.set_xlabel('PC1 [-] VarExp=%1.2f' %pcaObj.explained_variance_ratio_[0])\n",
        "ax.set_ylabel('PC2 [-] VarExp=%1.2f' %pcaObj.explained_variance_ratio_[1])\n",
        "ax.set_zlabel('PC3 [-] VarExp=%1.2f' %pcaObj.explained_variance_ratio_[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocTu2f55R0on"
      },
      "source": [
        "To see how many PCs we should reduce our sample to, let's calculate the percentage of variance explained by PCs from 2 up to 139 (half of our sample) - this is called sensitivity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkqFNJa0HQPi"
      },
      "outputs": [],
      "source": [
        "pcaObjDummy = PCA(n_components=139)\n",
        "pcDummy = pcaObjDummy.fit_transform(standWVS)\n",
        "VarianceExplained = pcaObjDummy.explained_variance_\n",
        "totVarianceExplained = pcaObjDummy.explained_variance_ratio_.cumsum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAvetUHSPus"
      },
      "source": [
        "And now let's check how 139 PCs do."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totVarianceExplained"
      ],
      "metadata": {
        "id": "371zhmroIuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAAs3f9xSnrh"
      },
      "source": [
        "Another way to pick the number of components is by using the *elbow method*.\n",
        "That is, we plot the number of PCs over the total variance and look for an *elbow* (the place where the explained variation begins to slow) in the plot to indicate how many PCs is appropriate to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v15PQg0zHVnV"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=200)\n",
        "plt.plot(range(1, 140), VarianceExplained, marker='o')\n",
        "plt.title('Sensitivity Analysis for PCA')\n",
        "plt.xlabel('Number of PCs')\n",
        "plt.ylabel('Variance [-]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBMX_yzFTRsJ"
      },
      "source": [
        "We can see a *elbow* around the 5th component indicating a decrease in the explained variation. Let's take a look more closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkM_qNQUuZU7"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=200)\n",
        "plt.plot(range(1, 11), VarianceExplained[:10], marker='o')\n",
        "plt.title('Sensitivity Analysis for PCA')\n",
        "plt.xlabel('Number of PCs')\n",
        "plt.ylabel('Eigenvalue (variance) [-]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR1xWFkcs37F"
      },
      "outputs": [],
      "source": [
        "totVarianceExplained[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URhTalWnuxo8"
      },
      "source": [
        "There is clearly a elbow in our plot. Yet, total variance explained by PC5 is only around 20%. Let's see how many components we may need to hit the 70-80% mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pZJSHITMOl_"
      },
      "outputs": [],
      "source": [
        "totVarianceExplained[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxEB9hzXMQ_p"
      },
      "source": [
        "139 components explain 78% of the variance. That's a lot of components but still we managed to reduce our dataframe by half the features (278 initially), not bad right?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDo0FKvNT8kr"
      },
      "source": [
        "## **K-means clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygIKNZnsUFr_"
      },
      "source": [
        "K-means clustering allows us to divide our dataset into a defined number of clusters (k) by assigning each observation to the cluster with the nearest mean.\n",
        "\n",
        "We combine k-means clustering with PCA to increase the data segmentation of our results. PCA reduced the number of features to fewer uncorrelated ones, reducing the noise in the data which can now be grouped more easily.\n",
        "\n",
        "To simplify the computation, we reduce our dataframe to two components in our application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK7l9g8LOnNt"
      },
      "source": [
        "Similarly to PCA, perform k-means clustering in `sklearn` we use a combination of the functions `KMeans()` and `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ3WrTkAPsUE"
      },
      "outputs": [],
      "source": [
        "inertias = []\n",
        "Nclusters = range(1,10)\n",
        "for i in Nclusters:\n",
        "    kmeans = KMeans(n_clusters=i)\n",
        "    kmeans.fit(prComp)\n",
        "    inertias.append(kmeans.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dukJXDqpPHZr"
      },
      "source": [
        "Similarly to PCA we can use *elbow method* to determine how many clusters we should keep. That is, we look for the *elbow* or the place where the inertia begins to slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVsD8ge5PvaQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=200)\n",
        "plt.plot(Nclusters, inertias, marker='o')\n",
        "plt.title('Elbow method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.xticks(Nclusters[1::4])\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfB9HLBdZPm5"
      },
      "source": [
        "In this case, 3 seems the right number of clusters to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykAkRrCOZu3q"
      },
      "source": [
        "Let's now combine the two methods and take a look at our clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1E8E-eMPxfW"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(prComp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the clusters centroids representing the average of each cluster."
      ],
      "metadata": {
        "id": "D_06aSEGQvxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = kmeans.cluster_centers_\n",
        "print(\"Cluster Centroids:\\n\", centroids)"
      ],
      "metadata": {
        "id": "-xXa9tylQr-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxyjIFlBPzim"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=200)\n",
        "plt.scatter(prComp[:,0], prComp[:,1], c= kmeans.labels_)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"X\", s=200, color=\"red\", label=\"Centroids\")\n",
        "plt.xlabel('PC1 [-]')\n",
        "plt.ylabel('PC2 [-]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our clusters are somehow separated (i.e., minimal overal is present) and centroids are apart from each other and located at the center of each cluster.\n",
        "\n",
        "Another way to check if clustering is successful is to compute its Silhouette score, measuring how well a point fits within its assigned cluster.\n",
        "\n",
        "* Silhouette Score = 1: Well-clustered (tight clusters, good separation)\n",
        "* Silhouette Score = 0: Overlapping clusters (unclear boundary)\n",
        "* Silhouette Score = -1: Wrong clustering (point assigned to the wrong group)"
      ],
      "metadata": {
        "id": "RrKfy3sARPhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sil_score = silhouette_score(prComp, kmeans.labels_)\n",
        "sil_score"
      ],
      "metadata": {
        "id": "DsTmZuPNPXRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, our score shows some overlap as the value is closer to 0 than to 1."
      ],
      "metadata": {
        "id": "dkW2uBA1Sctf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}